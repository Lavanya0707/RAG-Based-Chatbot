# ğŸ¤– RAG-Based Document Question Answering Chatbot

A **Retrieval-Augmented Generation (RAG) Chatbot** that allows users to ask questions from their documents and get **accurate, context-based answers** along with **source references**.  
The system uses **vector search + Large Language Models (LLMs)** to ensure answers are generated strictly from the provided documents.

---

## ğŸ“Œ Problem Statement

Traditional chatbots often generate answers without verifying the source, leading to **hallucinations** or incorrect information.  
This project solves that problem by:
- Retrieving **relevant document chunks** using vector similarity search
- Generating answers **only from retrieved context**
- Displaying **source references** for transparency

---

## ğŸš€ Features

- ğŸ” Semantic document search using **FAISS**
- ğŸ§  Context-aware answers using **Mistral LLM**
- ğŸ“„ Source citation with page numbers
- âš¡ Caching for repeated questions
- ğŸ“ Automatic MCQ generation from documents
- ğŸ–¥ï¸ User-friendly web interface using **Streamlit**

---

## ğŸ› ï¸ Tech Stack

| Category | Technologies |
|--------|-------------|
| Programming Language | Python |
| LLM | Ollama (Mistral) |
| Embeddings | HuggingFace (all-MiniLM-L6-v2) |
| Vector Database | FAISS |
| Framework | LangChain |
| Frontend | Streamlit |
| Deployment | Local (Ollama) |

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ Main code.ipynb # Notebook for experimentation and setup
â”œâ”€â”€ Backend code.py # RAG pipeline, retrieval & LLM logic
â”œâ”€â”€ UI code.py # Streamlit-based frontend
â”œâ”€â”€ vectorstore/ # FAISS vector database
â””â”€â”€ README.md



---

## âš™ï¸ How the System Works (Architecture)

1. **Document Embedding**
   - Documents are converted into vector embeddings using HuggingFace models
2. **Vector Storage**
   - Embeddings are stored locally using FAISS
3. **Query Processing**
   - User question is converted into an embedding
4. **Retrieval**
   - Top-k most relevant document chunks are retrieved
5. **Answer Generation**
   - LLM generates answers strictly using retrieved context
6. **Source Display**
   - Metadata (file & page number) shown as references

---

## ğŸ§  Backend Logic

The backend handles:
- Loading vector database
- Performing similarity search
- Prompt engineering for controlled LLM responses
- Caching repeated questions
- MCQ generation from documents  

ğŸ“Œ Implemented in: `Backend code.py`

---

## ğŸ–¥ï¸ User Interface

The frontend is built using **Streamlit** and provides:
- Text input for questions
- Answer display section
- Source references
- Loading spinner for better UX  

ğŸ“Œ Implemented in: `UI code.py`

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Prerequisites
- Python 3.9+
- Ollama installed
- Mistral model pulled:
```bash
ollama pull mistral


